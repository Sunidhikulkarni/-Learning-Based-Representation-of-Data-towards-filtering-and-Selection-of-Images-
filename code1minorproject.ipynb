{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8GOgPA3wWpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f2cdc8-2ab1-46e7-c8fe-f467797a464f"
      },
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AyXccTYw3J6"
      },
      "source": [
        "#importing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "from torchsummary import summary\n",
        "from IPython import display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_5j85-Qw3JO"
      },
      "source": [
        "#parameters\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNONxVkmw29H"
      },
      "source": [
        "#device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CayHQiUw28Z"
      },
      "source": [
        "#load data\n",
        "train_dataroot = \"/content/drive/MyDrive/minorproject_dataset_1/train\"\n",
        "dataset = dset.ImageFolder(root=train_dataroot,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
        "                                         shuffle=True)\n",
        "\n",
        "train_dataroot1 = \"/content/drive/MyDrive/minorproject_dataset_2/fore_ground_blur/train\"\n",
        "\n",
        "train_dataset1 = dset.ImageFolder(root=train_dataroot1,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "train_dataloader1 = torch.utils.data.DataLoader(train_dataset1, batch_size=1,\n",
        "                                         shuffle=True)\n",
        "\n",
        "\n",
        "test_dataroot1 = \"/content/drive/MyDrive/minorproject_dataset_2/fore_ground_blur/test\"\n",
        "\n",
        "test_dataset1 = dset.ImageFolder(root=test_dataroot1,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "test_dataloader1 = torch.utils.data.DataLoader(test_dataset1, batch_size=1,\n",
        "                                         shuffle=False)\n",
        "\n",
        "\n",
        "train_dataroot2 = \"/content/drive/MyDrive/minorproject_dataset_2/back_ground_blur/train\"\n",
        "\n",
        "train_dataset2 = dset.ImageFolder(root=train_dataroot2,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "train_dataloader2 = torch.utils.data.DataLoader(train_dataset2, batch_size=1,\n",
        "                                         shuffle=True)\n",
        "\n",
        "\n",
        "test_dataroot2 = \"/content/drive/MyDrive/minorproject_dataset_2/back_ground_blur/test\"\n",
        "\n",
        "test_dataset2 = dset.ImageFolder(root=test_dataroot2,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "test_dataloader2 = torch.utils.data.DataLoader(test_dataset2, batch_size=1,\n",
        "                                         shuffle=False)\n",
        "\n",
        "\n",
        "train_dataroot3 = \"/content/drive/MyDrive/minorproject_dataset_2/sharp/train\"\n",
        "\n",
        "train_dataset3 = dset.ImageFolder(root=train_dataroot2,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "train_dataloader3 = torch.utils.data.DataLoader(train_dataset2, batch_size=1,\n",
        "                                         shuffle=True)\n",
        "\n",
        "\n",
        "test_dataroot3 = \"/content/drive/MyDrive/minorproject_dataset_2/sharp/test\"\n",
        "test_dataset3 = dset.ImageFolder(root=test_dataroot2,\n",
        "                           transform=transforms.Compose([transforms.Resize(size=(128,128)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "test_dataloader3 = torch.utils.data.DataLoader(test_dataset2, batch_size=1,\n",
        "                                         shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgtg5-i4w23p"
      },
      "source": [
        "#Network Architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 9, kernel_size=(3,3),padding=1)\n",
        "        self.conv2 = nn.Conv2d(9, 27, kernel_size=(3,3),padding=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2), return_indices=True)\n",
        "        self.conv3 = nn.Conv2d(27, 81, kernel_size=(3,3),padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2), return_indices=True)\n",
        "        self.maxunpool1 = nn.MaxUnpool2d(kernel_size=(2,2))\n",
        "        self.unconv1 = nn.ConvTranspose2d(81,27,kernel_size=(3,3), padding=1)\n",
        "        self.maxunpool2 = nn.MaxUnpool2d(kernel_size=(2,2))\n",
        "        self.unconv2 = nn.ConvTranspose2d(27,9,kernel_size=(3,3), padding=1)\n",
        "        self.unconv3 = nn.ConvTranspose2d(9,3,kernel_size=(3,3),padding=1)\n",
        "        \n",
        "\n",
        "    def Encoder(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        x= self.conv2(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        x,indices1 = self.maxpool1(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        x= self.conv3(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        x,indices2 = self.maxpool2(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        return x,indices1,indices2\n",
        "\n",
        "    def Decoder(self, x,indices1,indices2):\n",
        "        x = self.maxunpool2(x, indices2)\n",
        "        x= nn.Tanh()(x)\n",
        "        x = self.unconv1(x)\n",
        "        x= nn.Tanh()(x)  \n",
        "        x = self.maxunpool1(x,indices1)\n",
        "        x= nn.Tanh()(x)\n",
        "        x = self.unconv2(x)\n",
        "        x= nn.Tanh()(x)\n",
        "        x = self.unconv3(x)\n",
        "        return x\n",
        "   ############################################ \n",
        "  \n",
        "    def forward(self, x):\n",
        "        latent,i1,i2 = self.Encoder(x)\n",
        "        output = self.Decoder(latent,i1,i2)\n",
        "        return output\n",
        "        \n",
        "\n",
        "    ##########################################################################################################################################\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIX5JOnmw22u",
        "outputId": "1b5e66ac-dca5-4483-af65-1741d4bd9707"
      },
      "source": [
        "#printing the number of parameters\n",
        "model = Autoencoder().to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 44382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkY1fTRzw2yA",
        "outputId": "ee84cd1c-52ef-4db5-dbcd-dbe79672d1b5"
      },
      "source": [
        "#training\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-5) \n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch , _ in dataloader:\n",
        "      \n",
        "        image_batch = image_batch.to(device)\n",
        "        \n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = model.forward(image_batch)\n",
        "        #L,I1,I2= model.Encoder(image_batch)\n",
        "        #image_batch_recon=model.Decoder(L,I1,I2)\n",
        "        # reconstruction error\n",
        "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:693: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 1] average reconstruction error: 0.008178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "J8Ij7LCew2xI",
        "outputId": "01e70362-ff99-46c2-a305-a884d1f5aa1f"
      },
      "source": [
        "#plotting graph\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2oXnqVkgQbY"
      },
      "source": [
        "FILE = \"/content/drive/My Drive/minorproject_model_save/trained_model.pt\"\n",
        "torch.save(model, FILE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q5o4vA7XdwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "26d78161-fbed-4beb-ba3e-e6def13d6e13"
      },
      "source": [
        "model.eval()\n",
        "def Load(dataloader,latentpath,I1_path,I2_path):\n",
        "    count=0\n",
        "    for image_batch, _ in dataloader:  \n",
        "        with torch.no_grad():\n",
        "            image_batch = image_batch.to(device)\n",
        "            # autoencoder reconstruction\n",
        "            L,I1,I2 = model.Encoder(image_batch)\n",
        "            \n",
        "            b=len(image_batch)\n",
        "            for i in range(b):\n",
        "                count+=1\n",
        "                L_image=L[i] \n",
        "                I1_image=I1[i] \n",
        "                I2_image=I2[i] \n",
        "\n",
        "                torch.save(L_image.unsqueeze(0), latentpath.format(count)) \n",
        "                #torch.save(I1_image.unsqueeze(0), I1_path.format(count)) \n",
        "                #torch.save(I2_image.unsqueeze(0), I2_path.format(count)) \n",
        "    \n",
        "latent_path_good_test=\"/content/drive/MyDrive/minor_project_latent/test/blurs/fore_ground_blur/Latent_{}.txt\"\n",
        "#latent_path_good_test=\"/content/drive/MyDrive/drill_save/test/drilling/good/Latent_{}.txt\"\n",
        "I1_path_good_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/fore_ground_blur/I1_{}.txt\"\n",
        "I2_path_good_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/fore_ground_blur/I2_{}.txt\"\n",
        "Load(test_dataloader1,latent_path_good_test,I1_path_good_test,I2_path_good_test)\n",
        "\n",
        "latent_path_average_test=\"/content/drive/MyDrive/minor_project_latent/test/blurs/back_ground_blur/Latent_{}.txt\"\n",
        "#latent_path_average_test=\"/content/drive/MyDrive/drill_save/test/drilling/average/Latent_{}.txt\"\n",
        "I1_path_average_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/back_ground_blur/I1_{}.txt\"\n",
        "I2_path_average_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/back_ground_blur/I2_{}.txt\"\n",
        "Load(test_dataloader2,latent_path_average_test,I1_path_average_test,I2_path_average_test)\n",
        "\n",
        "latent_path_bad_test=\"/content/drive/MyDrive/minor_project_latent/test/blurs/sharp/Latent_{}.txt\"\n",
        "#latent_path_bad_test=\"/content/drive/MyDrive/drill_save/test/drilling/bad/Latent_{}.txt\"\n",
        "I1_path_bad_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/sharp/I1_{}.txt\"\n",
        "I2_path_bad_test=\"/content/drive/MyDrive/minor_project_save/test/blurs/sharp/I2_{}.txt\"\n",
        "Load(test_dataloader3,latent_path_bad_test,I1_path_bad_test,I2_path_bad_test)\n",
        "\n",
        "latent_path_good_train=\"/content/drive/MyDrive/minor_project_latent/train/blurs/fore_ground_blur/Latent_{}.txt\"\n",
        "#latent_path_good_train=\"/content/drive/MyDrive/drill_save/train/drilling/good/Latent_{}.txt\"\n",
        "I1_path_good_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/fore_ground_blur/I1_{}.txt\"\n",
        "I2_path_good_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/fore_ground_blur/I2_{}.txt\"\n",
        "Load(train_dataloader1,latent_path_good_train,I1_path_good_train,I2_path_good_train)\n",
        "\n",
        "latent_path_average_train=\"/content/drive/MyDrive/minor_project_latent/train/blurs/back_ground_blur/Latent_{}.txt\"\n",
        "#latent_path_average_train=\"/content/drive/MyDrive/drill_save/train/drilling/average/Latent_{}.txt\"\n",
        "I1_path_average_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/back_ground_blur/I1_{}.txt\"\n",
        "I2_path_average_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/back_ground_blur/I2_{}.txt\"\n",
        "Load(train_dataloader2,latent_path_average_train,I1_path_average_train,I2_path_average_train)\n",
        "\n",
        "latent_path_bad_train=\"/content/drive/MyDrive/minor_project_latent/train/blurs/sharp/Latent_{}.txt\"\n",
        "#latent_path_bad_train=\"/content/drive/MyDrive/drill_save/train/drilling/bad/Latent_{}.txt\"\n",
        "I1_path_bad_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/sharp/I1_{}.txt\"\n",
        "I2_path_bad_train=\"/content/drive/MyDrive/minor_project_save/train/blurs/sharp/I2_{}.txt\"\n",
        "Load(train_dataloader3,latent_path_bad_train,I1_path_bad_train,I2_path_bad_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-13d7fc766a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mI1_path_average_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/minor_project_save/train/blurs/back_ground_blur/I1_{}.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mI2_path_average_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/minor_project_save/train/blurs/back_ground_blur/I2_{}.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_path_average_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI1_path_average_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI2_path_average_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mlatent_path_bad_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/minor_project_latent/train/blurs/sharp/Latent_{}.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-13d7fc766a85>\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(dataloader, latentpath, I1_path, I2_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatentpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI1_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m    231\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Lt5oo5doYOh5",
        "outputId": "14dc8510-b1fd-4e4e-e577-54b1bde19da7"
      },
      "source": [
        "'''\n",
        "#testing again\n",
        "#set to evaluation mode\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "    \n",
        "# autoencoder reconstruction\n",
        "\n",
        "print('Reconstructed images')\n",
        "\n",
        "with torch.no_grad():\n",
        "  for count in range(1,25):\n",
        "\n",
        "    loadlatent=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/Latent_{}.txt\".format(count)))\n",
        "    loadlatent=torch.unsqueeze(torch.reshape(loadlatent,(81,32,32)),0)\n",
        "    loadlatent=loadlatent.type(torch.float32)\n",
        "\n",
        "    loadI1=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/I1_{}.txt\".format(count)))\n",
        "    loadI1=torch.unsqueeze(torch.reshape(loadI1,(27,64,64)),0)\n",
        "    loadI1=loadI1.type(torch.int64)\n",
        "\n",
        "    loadI2=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/I2_{}.txt\".format(count)))\n",
        "    loadI2=torch.unsqueeze(torch.reshape(loadI2,(81,32,32)),0)\n",
        "    loadI2=loadI2.type(torch.int64)\n",
        "\n",
        "    image_batch_recon=model.Decoder(loadlatent, loadI1, loadI2)\n",
        "\n",
        "    # reconstruction error\n",
        "    loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "\n",
        "    test_loss_avg += loss.item()\n",
        "    num_batches += 1\n",
        "\n",
        "    image_batch_recon = image_batch_recon.cpu()\n",
        "    image_batch_recon = to_img(image_batch_recon)\n",
        "    np_imagegrid = torchvision.utils.make_grid(image_batch_recon[:24], 10, 5).numpy()\n",
        "    plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "      \n",
        "  test_loss_avg /= num_batches\n",
        "  print('average reconstruction error: %f' % (test_loss_avg))\n",
        "'''\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#testing again\\n#set to evaluation mode\\nplt.ion()\\n\\nimport torchvision.utils\\n\\nmodel.eval()\\n\\ndef to_img(x):\\n    x = 0.5 * (x + 1)\\n    x = x.clamp(0, 1)\\n    return x\\n    \\n# autoencoder reconstruction\\n\\nprint(\\'Reconstructed images\\')\\n\\nwith torch.no_grad():\\n  for count in range(1,25):\\n\\n    loadlatent=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/Latent_{}.txt\".format(count)))\\n    loadlatent=torch.unsqueeze(torch.reshape(loadlatent,(81,32,32)),0)\\n    loadlatent=loadlatent.type(torch.float32)\\n\\n    loadI1=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/I1_{}.txt\".format(count)))\\n    loadI1=torch.unsqueeze(torch.reshape(loadI1,(27,64,64)),0)\\n    loadI1=loadI1.type(torch.int64)\\n\\n    loadI2=torch.from_numpy(np.loadtxt(\"/content/drive/My Drive/Encoded_values/I2_{}.txt\".format(count)))\\n    loadI2=torch.unsqueeze(torch.reshape(loadI2,(81,32,32)),0)\\n    loadI2=loadI2.type(torch.int64)\\n\\n    image_batch_recon=model.Decoder(loadlatent, loadI1, loadI2)\\n\\n    # reconstruction error\\n    loss = F.mse_loss(image_batch_recon, image_batch)\\n\\n    test_loss_avg += loss.item()\\n    num_batches += 1\\n\\n    image_batch_recon = image_batch_recon.cpu()\\n    image_batch_recon = to_img(image_batch_recon)\\n    np_imagegrid = torchvision.utils.make_grid(image_batch_recon[:24], 10, 5).numpy()\\n    plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\\n    plt.show()\\n\\n      \\n  test_loss_avg /= num_batches\\n  print(\\'average reconstruction error: %f\\' % (test_loss_avg))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXtvYEqog3bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "3f3b0206-0c7e-41ae-ec71-d4af54eeacfa"
      },
      "source": [
        "'''#testing\n",
        "#set to evaluation mode\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def loadLatent(dataloader, latent_path, I1_path, I2_path):\n",
        "\n",
        "  count=0\n",
        "  test_loss_avg, num_batches = 0, 0\n",
        "\n",
        "  for image_batch, _ in dataloader:  \n",
        "      with torch.no_grad():\n",
        "          image_batch = image_batch.to(device)\n",
        "          # autoencoder reconstruction\n",
        "          L,I1,I2 = model.Encoder(image_batch)\n",
        "          \n",
        "          b=len(image_batch)\n",
        "          for i in range(b):\n",
        "              count+=1\n",
        "              L_image=torch.reshape(L[i] , (L.shape[1]*L.shape[2] , L.shape[3] ))\n",
        "              I1_image=torch.reshape(I1[i] , (I1.shape[1]*I1.shape[2] , I1.shape[3] ))\n",
        "              I2_image=torch.reshape(I2[i] , (I2.shape[1]*I2.shape[2] , I2.shape[3] ))\n",
        "                  \n",
        "              np.savetxt(latent_path, L_image )\n",
        "              np.savetxt(I1_path, I1_image )\n",
        "              np.savetxt(I2_path, I2_image )\n",
        "        \n",
        "test_latent = \"/content/drive/My Drive/Encoded_values/Latent_{}.txt\".format(count)\n",
        "test_I1= \"/content/drive/My Drive/Encoded_values/I1_{}.txt\".format(count)\n",
        "test_I2= \"/content/drive/My Drive/Encoded_values/I2_{}.txt\".format(count)\n",
        "loadLatent(test_dataloader,test_latent,test_I1,test_I2)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#testing\\n#set to evaluation mode\\n\\nimport numpy as np\\n\\nmodel.eval()\\n\\ndef loadLatent(dataloader, latent_path, I1_path, I2_path):\\n\\n  count=0\\n  test_loss_avg, num_batches = 0, 0\\n\\n  for image_batch, _ in dataloader:  \\n      with torch.no_grad():\\n          image_batch = image_batch.to(device)\\n          # autoencoder reconstruction\\n          L,I1,I2 = model.Encoder(image_batch)\\n          \\n          b=len(image_batch)\\n          for i in range(b):\\n              count+=1\\n              L_image=torch.reshape(L[i] , (L.shape[1]*L.shape[2] , L.shape[3] ))\\n              I1_image=torch.reshape(I1[i] , (I1.shape[1]*I1.shape[2] , I1.shape[3] ))\\n              I2_image=torch.reshape(I2[i] , (I2.shape[1]*I2.shape[2] , I2.shape[3] ))\\n                  \\n              np.savetxt(latent_path, L_image )\\n              np.savetxt(I1_path, I1_image )\\n              np.savetxt(I2_path, I2_image )\\n        \\ntest_latent = \"/content/drive/My Drive/Encoded_values/Latent_{}.txt\".format(count)\\ntest_I1= \"/content/drive/My Drive/Encoded_values/I1_{}.txt\".format(count)\\ntest_I2= \"/content/drive/My Drive/Encoded_values/I2_{}.txt\".format(count)\\nloadLatent(test_dataloader,test_latent,test_I1,test_I2)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}